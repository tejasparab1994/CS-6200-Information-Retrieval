As per the question for the task 2 of the assignment, we had to build a focused crawler. Unlike, the one built in Task 1-E, the crawler in Task 2 had to only crawl links matching the keyword from the seed page and the later depths.
    As per the example provided in the assignment question, for a keyword 'rain', "falling_rain", "rain_fall", "rainband", "Rain", "rains" should be considered as valid variations, whereas "grains, "ukraine" should not be considered as valid matches.
   In order to execute, the given problem statement, the main function in the code (main_crawler()) stores the seed value. Using a list, the value of seed is passed to the function responsible to get all the links present on the page passed to it. This function; crawl_all(url) takes a string url as an input and returns a list containing all the links satisfying the criteria mentioned by the problem. Links containing ":" or links redirecting to the same page have been avoided and not been added to the list. In case, there are any repeated or duplicated links in the list, the link is cleared of all the duplicates through the list(OrderedDict.fromkeys()) function.
   On receiving the link back in the main function, the seed url is added into the visited list. This visited list would be used later in order to maintain a record of which lists have been visited, to search and recognize unique urls. A for loop is run on the first link of the links obtained from the seed url. This link goes through the same procedure as the seed url in depth 1. The duplicated links are removed and the unique urls are stored in the list. This url, through which we scrapped the data, would now be added into the visited lists.
   A counter is maintained in order to keep a tab on number of lists that have been scrapped.In order to avoid links similar to our keyword but not relevant, a function rain_check(url,key). The function uses regular expressions to filter irrelevant links.
  The raw documents are created using the raw_doc(url) function. The remove_dup(duplist) function is used to remove duplicate links in lists.
